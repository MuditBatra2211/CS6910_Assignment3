import tensorflow as tf

physical_devices = tf.config.experimental.list_physical_devices('GPU')
# assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
# config = tf.config.experimental.set_memory_growth(physical_devices[0], True)

from tensorflow.keras import  models, optimizers, layers, activations
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, LSTM, SimpleRNN, GRU, Dense, Embedding,RNN
import matplotlib.pyplot as plt
import numpy as np
import codecs
%pip install wandb -q
import wandb
from wandb.keras import WandbCallback
#==================================================================================================#
import wandb
wandb.login()
default_configs = {"architecture" : "LSTM",
                   "batchSize" : 32,
                   "dropout" : 0.2,
                   "epochs" : 10,
                   "encoderLayers" : 2,
                   "decoderLayers" : 1,
                   "hiddenSize" : 64,
                   "embeddingSize" : 32}
run = wandb.init(project='CS6910_assignment3_v1', config=default_configs)
config = wandb.config

#==================================================================================================#

class NLP:
    xTrain = []
    yTrain = []
    xValid = []
    yValid = []
    xTest = []
    yTest =[]
    
    def __init__(self):

        self.trainingExamples = len(self.xTrain)
        self.validExamples = len(self.xValid)

        self.architecture = config.architecture
        self.batchSize = config.batchSize
        self.dropout = config.dropout
        self.epochs = config.epochs

        self.encoderLayers = config.encoderLayers
        self.decoderLayers = config.decoderLayers
        self.hiddenSize = config.hiddenSize
        self.embeddingSize = config.embeddingSize


    def getDictionary(self):
        
        enText = self.xTrain
        hiText = self.yTrain

        self.charText_hi = set(''.join(hiText))
        self.charText_hi.add(' ')
        self.charText_en = set(''.join(enText))
        self.charText_en.add(' ')

        self.int2char_hi = dict(enumerate(self.charText_hi))
        self.int2char_en = dict(enumerate(self.charText_en))

        self.char2int_hi = {char: ind for ind, char in self.int2char_hi.items()}
        self.char2int_en = {char: ind for ind, char in self.int2char_en.items()}

        self.vocabSize_hi = len(self.int2char_hi)
        self.vocabSize_en = len(self.int2char_en)

        self.inTrainMaxlen = len(max(self.xTrain, key=len))
        self.outTrainMaxlen = len(max(self.yTrain, key=len))

    def word2vec(self,dataset):
        
        if (dataset == 'train'):
            enText = self.xTrain
            hiText = self.yTrain

        elif(dataset == 'valid'):
            enText = self.xValid
            hiText = self.yValid 
        else:
            enText = self.xTest
            hiText = self.yTest

        
        self.encoderInput = np.zeros((len(enText), self.inTrainMaxlen), dtype="float32")
        self.decoderInput = np.zeros((len(hiText), self.outTrainMaxlen, self.vocabSize_hi), dtype="float32")
        self.decoderOutput = np.zeros((len(hiText), self.outTrainMaxlen, self.vocabSize_hi), dtype="float32")

        for i, (x, y) in enumerate(zip(enText, hiText)):
            for t, char in enumerate(x):
                self.encoderInput[i, t] = self.char2int_en[char]
                
            self.encoderInput[i, t + 1 :] = self.char2int_en[" "]

            for t, char in enumerate(y):
                # decoder_target_data is ahead of decoder_input_data by one timestep
                self.decoderInput[i, t, self.char2int_hi[char]] = 1.0
                if t > 0:
                    self.decoderOutput[i, t - 1, self.char2int_hi[char]] = 1.0
                    
            self.decoderInput[i, t + 1 :, self.char2int_hi[" "]] = 1.0
            self.decoderOutput[i, t:, self.char2int_hi[" "]] = 1.0  


    def toggleFunc(self):
        if self.architecture == 'GRU':
            return GRU
        elif self.architecture == 'RNN':
            return SimpleRNN
        elif self.architecture == 'LSTM':
            return LSTM
        else:
            print('Please enter correct architecture')
            exit()

    def encoderDecoderModels(self):

        if (self.architecture == 'LSTM'):

            encoderInputs = Input(shape=(None,))
            encoderEmbedding = layers.Embedding(input_dim=self.vocabSize_en, output_dim=self.embeddingSize)(encoderInputs)
            tempEncoder = encoderEmbedding
            
            for i in range(self.encoderLayers-1):
                tempEncoder = self.toggleFunc()(self.hiddenSize, return_state=True, return_sequences=True,dropout=self.dropout,name = 'encoderRNN'+str(i))(tempEncoder)
            encoderOutputs, state_h, state_c = self.toggleFunc()(self.hiddenSize, return_state=True, dropout=self.dropout,name = 'encoderRNNfinal')(tempEncoder)
            encoderStates = [state_h, state_c]
        
        else:
            encoderInputs = Input(shape=(None,))
            encoderEmbedding = layers.Embedding(input_dim=self.vocabSize_en, output_dim=self.embeddingSize)(encoderInputs)
            tempEncoder = encoderEmbedding
            
            for i in range(self.encoderLayers-1):
                tempEncoder = self.toggleFunc()(self.hiddenSize, return_state=True, return_sequences=True,dropout=self.dropout,name ='encoderRNN'+str(i))(tempEncoder)
            encoderOutputs, state_h = self.toggleFunc()(self.hiddenSize, return_state=True, dropout=self.dropout,name ='encoderRNNfinal')(tempEncoder)
            encoderStates = [state_h]

        decoderInputs = Input(shape=(None,self.vocabSize_hi))
        tempDecoder = decoderInputs
        tempDecoder = self.toggleFunc()(self.hiddenSize, return_sequences=True, return_state=True,dropout=self.dropout)(tempDecoder,initial_state=encoderStates)
        for i in range(self.decoderLayers-1):
            tempDecoder = self.toggleFunc()(self.hiddenSize, return_sequences=True, return_state=True,dropout=self.dropout)(tempDecoder)
        
        if (self.architecture == 'LSTM'):
            decoderOutputs, _, _ = tempDecoder
        else:
            decoderOutputs, _ = tempDecoder

        decoderDense = Dense(self.vocabSize_hi, activation="softmax")
        decoderOutputs = decoderDense(decoderOutputs)

        # decoder_attention = attention.SequenceAttention(similarity="additive", name="attention")
        # decoderOutputs = decoder_attention([decoderOutputs, encoderOutputs])
        model = Model([encoderInputs, decoderInputs], decoderOutputs)

        return model
#====================================================================================================================#
def getInputData(filename):
  hiText,enText = [],[]
  with codecs.open(filename, encoding='utf-8') as f:
    for row in f:
      hiWord, enWord, _ = row.split("\t")
      hiWord = "<" + hiWord + ">"
      hiText.append(hiWord)
      enText.append(enWord)
  
  return enText,hiText
  
xTrain,yTrain = getInputData('train.txt')
xValid,yValid = getInputData('valid.txt')
xTest,yTest = getInputData('test.txt')
NLP = NLP()

NLP.xTrain = xTrain
NLP.yTrain = yTrain
NLP.xValid = xValid
NLP.yValid = yValid
NLP.xTest = xTest
NLP.yTest = yTest
#====================================================================================================================#

NLP.getDictionary()
NLP.word2vec('train')
trainEncoderInput,trainDecoderInput,trainDecoderOutput = NLP.encoderInput,NLP.decoderInput,NLP.decoderOutput
model = NLP.encoderDecoderModels()
NLP.word2vec('valid')
validEncoderInput,validDecoderInput,validDecoderOutput = NLP.encoderInput,NLP.decoderInput,NLP.decoderOutput
NLP.word2vec('test')
testEncoderInput,_,_ = NLP.encoderInput,NLP.decoderInput,NLP.decoderOutput

#====================================================================================================================#

model.compile(optimizer="rmsprop", loss="categorical_crossentropy", metrics=["accuracy"])
model.fit([trainEncoderInput, trainDecoderInput],
    trainDecoderOutput,
    batch_size=NLP.batchSize,
    epochs=NLP.epochs,
    validation_data=([validEncoderInput, validDecoderInput],validDecoderOutput),
    callbacks=[WandbCallback()]
)

#====================================================================================================================#

encoderInputs = model.input[0]  # input_1
if (NLP.architecture=='LSTM'):
    encoderOutputs, state_h_enc, state_c_enc = model.layers[NLP.encoderLayers+2].output  # lstm_1
    encoderStates = [state_h_enc, state_c_enc]
else:
    encoderOutputs, state_h_enc = model.layers[NLP.encoderLayers+2].output  # lstm_1
    encoderStates = [state_h_enc]
encoder_model_1 = Model(encoderInputs, encoderStates)

decoderInputs = model.input[1]  # input_2
if (NLP.architecture=='LSTM'):
    decoder_state_input_h = Input(shape=(NLP.hiddenSize,))
    decoder_state_input_c = Input(shape=(NLP.hiddenSize,))
    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
else:
    decoder_state_input_h = Input(shape=(NLP.hiddenSize,))
    decoder_states_inputs = [decoder_state_input_h]


decoderStartingLayer = NLP.encoderLayers+3
for i in range(NLP.decoderLayers):
    decodingCell = model.layers[decoderStartingLayer+i]
    if (i==0):
        decoderMemory = decodingCell(decoderInputs, initial_state=decoder_states_inputs)
    else :
        decoderMemory = decodingCell(decoderMemory)

if (NLP.architecture=='LSTM'):    
    decoderOutputs, state_h_dec, state_c_dec = decoderMemory
    decoderStates = [state_h_dec, state_c_dec]
else:
    decoderOutputs, state_h_dec = decoderMemory
    decoderStates = [state_h_dec]
            
decoderDense = model.layers[decoderStartingLayer + NLP.decoderLayers]
decoderOutputs = decoderDense(decoderOutputs)
decoder_model_1 = Model([decoderInputs] + decoder_states_inputs, [decoderOutputs] + decoderStates)

#====================================================================================================================#

def decode_sequence(input_seq):
    # Encode the input as state vectors.
    states_value = encoder_model_1.predict(input_seq)

    # Generate empty target sequence of length 1.
    target_seq = np.zeros((1, 1, NLP.vocabSize_hi))
    # Populate the first character of target sequence with the start character.
    target_seq[0, 0, NLP.char2int_hi["<"]] = 1.0

    # Sampling loop for a batch of sequences
    # (to simplify, here we assume a batch of size 1).
    stop_condition = False
    decoded_sentence = ""
    while not stop_condition:
        output_tokens, h, c = decoder_model_1.predict([target_seq] + states_value)
        # print(output_tokens.shape)

        # Sample a token
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = NLP.int2char_hi[sampled_token_index]
        decoded_sentence += sampled_char

        # Exit condition: either hit max length
        # or find stop character.
        if sampled_char == ">" or len(decoded_sentence) > NLP.outTrainMaxlen:
            stop_condition = True

        # Update the target sequence (of length 1).
        target_seq = np.zeros((1, 1, NLP.vocabSize_hi))
        target_seq[0, 0, sampled_token_index] = 1.0

        # Update states
        states_value = [h, c]
    return decoded_sentence
    
    #====================================================================================================================#
    
    ref = []
hypo = []
for seq_index in range(100):
    # Take one sequence (part of the training set)
    # for trying out decoding.
    input_seq = testEncoderInput[seq_index : seq_index + 1]
    decoded_sentence = decode_sequence(input_seq)
    decoded_label = decoded_sentence.replace('>','')
    print("-")
    print("Input sentence:", xTest[seq_index])
    print("Decoded sentence:", decoded_label)
    hypo.append(decoded_label) 
    reference_sentence = yTest[seq_index]
    temp1 = reference_sentence.replace('>','')
    reference_label = temp1.replace('<','')
    print("Reference sentence:", reference_label)
    
    ref.append(reference_label)

print('hypo: ', hypo)
print('ref: ', ref)

#====================================================================================================================#

import fastwer
# # Corpus-Level WER: 40.0
print(fastwer.score(hypo, ref))
# # Corpus-Level CER: 25.5814
print(fastwer.score(hypo, ref, char_level=True))
